{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_kernel(D, psi=8):\n",
    "    locs = np.linspace(0,D/psi,D).reshape(-1,1)\n",
    "    K = pairwise_kernels(locs, metric='rbf')\n",
    "    return K\n",
    "\n",
    "def construct_covariance(K):\n",
    "    \"\"\"Sigma_{ZZ} = E[Sigma]\"\"\"\n",
    "    cov = K.copy()\n",
    "    magic_const = 35 * np.sqrt(np.pi) / 64\n",
    "    for i in range(cov.shape[0]):\n",
    "        for j in range(cov.shape[1]):\n",
    "            if i % 2 != j % 2:\n",
    "                cov[i,j] *= magic_const\n",
    "    return cov\n",
    "\n",
    "def construct_cubed_covariance(K):\n",
    "    \"\"\"E[Sigma^3]\"\"\"\n",
    "    cov3 = K.copy()**3\n",
    "    magic_const_odd = 8 / 3\n",
    "    magic_const_diff = 5 * np.sqrt(np.pi) / 8\n",
    "    for i in range(cov.shape[0]):\n",
    "        for j in range(cov.shape[1]):\n",
    "            if i % 2 != j % 2:\n",
    "                cov3[i,j] *= magic_const_diff\n",
    "            elif (i+1) % 2 == 1:\n",
    "                cov3[i,j] *= magic_const_odd\n",
    "    return cov3\n",
    "\n",
    "def construct_Sigma_ff(K):\n",
    "    cov = construct_covariance(K)\n",
    "    cov3 = construct_cubed_covariance(K)\n",
    "    return 3 * (3*cov + 2*cov3)\n",
    "\n",
    "def construct_Sigma_Zf(K):\n",
    "    Sigma_Zf = 3*K\n",
    "    magic_const_odd = 4 / 3\n",
    "    magic_const_even_odd = 5 * np.sqrt(np.pi) / 8\n",
    "    magic_const_odd_even = 35 * np.sqrt(np.pi) / 64\n",
    "    for i in range(cov.shape[0]):\n",
    "        for j in range(cov.shape[1]):\n",
    "            if (i+1) % 2 == 1:\n",
    "                if (j+1) % 2 == 1:\n",
    "                    Sigma_Zf[i,j] *= magic_const_odd\n",
    "                else:\n",
    "                    Sigma_Zf[i,j] *= magic_const_odd_even\n",
    "            elif (j+1) % 2 == 1:\n",
    "                Sigma_Zf[i,j] *= magic_const_even_odd\n",
    "    return Sigma_Zf\n",
    "\n",
    "def nonzero_sparse_inds(k, D):\n",
    "    inds = np.array([(i*(D+.5))//(k+1) for i in range(1,k+1)], dtype=int) - 1\n",
    "    return inds\n",
    "\n",
    "def get_beta(sparsity, D):\n",
    "    if sparsity == 'dense':\n",
    "        return 2**(2-np.arange(D)/2)\n",
    "    if sparsity[-6:] != 'sparse':\n",
    "        sys.exit('invalid sparsity type {}'.format(sparsity))\n",
    "    k = int(sparsity[:-6])\n",
    "    beta = np.zeros(D)\n",
    "    inds = nonzero_sparse_inds(k, D)\n",
    "    beta[inds] = 1\n",
    "    return beta, inds+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-sparse-nonlinear with D = 10 and k_max = 1\n",
      "best expected log-likelihood: -2.5702310797016956\n",
      "data-generating beta components:  5\n",
      "\n",
      "model  best ELL - ELL\n",
      " 5     0\n",
      " 7     0.103\n",
      " 3     0.103\n",
      " 6     0.215\n",
      " 4     0.215\n",
      " 9     0.2761\n",
      " 1     0.2761\n",
      " 8     0.2993\n",
      " 2     0.2993\n",
      "10     0.3894\n"
     ]
    }
   ],
   "source": [
    "D = 10\n",
    "K = construct_kernel(D)\n",
    "cov = construct_covariance(K)\n",
    "Sigma_ff = construct_Sigma_ff(K)\n",
    "Sigma_Zf = construct_Sigma_Zf(K)\n",
    "beta, comps = get_beta('1sparse', D)\n",
    "bSffbp1 = 1 + beta.dot(Sigma_ff).dot(beta)\n",
    "ELL = lambda s: -.5* (np.log(bSffbp1 - s) + 1 + np.log(2*np.pi))\n",
    "SZfb = Sigma_Zf.dot(beta)\n",
    "\n",
    "# larger score = larger expected log-likelihood\n",
    "scores = SZfb**2 / np.diag(cov)\n",
    "scores_arr = np.array([(i+1,s) for i, s in enumerate(scores)], \n",
    "                      dtype=[('ind', int), ('score', float)])\n",
    "scores_arr.sort(order='score')\n",
    "scores_arr = scores_arr[::-1]\n",
    "best_ELL = ELL(scores_arr[0][-1])\n",
    "print('1-sparse-nonlinear with D = {} and k_max = 1'.format(D))\n",
    "print('best expected log-likelihood:', best_ELL)\n",
    "print('data-generating beta components: ', *comps)\n",
    "print()\n",
    "print('model  best ELL - ELL')\n",
    "for i, s in scores_arr:\n",
    "    print('{:2d}     {:.4g}'.format(i, best_ELL - ELL(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-sparse-nonlinear with D = 10, k_max = 2, psi = 8\n",
      "best expected log-likelihood: -2.3760251587980212\n",
      "data-generating beta components:  5\n",
      "\n",
      " model   best ELL - ELL\n",
      " 7   8   0\n",
      " 2   3   1.332e-15\n",
      " 5   6   0.01048\n",
      " 4   5   0.01048\n",
      " 5   8   0.1609\n",
      " 2   5   0.1609\n",
      " 7  10   0.1693\n",
      " 5  10   0.185\n",
      " 5   9   0.1942\n",
      " 5   7   0.1942\n",
      " 3   5   0.1942\n",
      " 1   5   0.1942\n",
      " 3   7   0.2036\n",
      " 7   9   0.2237\n",
      " 1   3   0.2237\n",
      " 3   9   0.2285\n",
      " 1   7   0.2285\n",
      " 6   7   0.2675\n",
      " 3   4   0.2675\n",
      " 9  10   0.2703\n",
      "\n",
      "\n",
      "2-sparse-nonlinear with D = 20, k_max = 2, psi = 8\n",
      "best expected log-likelihood: -2.781849754264274\n",
      "data-generating beta components:  6 13\n",
      "\n",
      " model   best ELL - ELL\n",
      " 7  13   0\n",
      " 5  13   0.0004632\n",
      "11  12   0.006617\n",
      " 7  15   0.0143\n",
      " 6  13   0.0205\n",
      " 3  11   0.02102\n",
      " 9  15   0.02119\n",
      " 4  11   0.02176\n",
      " 2  11   0.02433\n",
      " 5  11   0.02447\n",
      " 4  13   0.02548\n",
      "13  14   0.02591\n",
      " 9  17   0.03205\n",
      " 9  13   0.03556\n",
      " 1  11   0.03578\n",
      " 6  11   0.0428\n",
      " 7  11   0.04311\n",
      " 3  13   0.04459\n",
      " 9  19   0.06038\n",
      " 9  11   0.06379\n",
      "\n",
      "\n",
      "2-sparse-nonlinear with D = 20, k_max = 2, psi = 4\n",
      "best expected log-likelihood: -2.8026141500929023\n",
      "data-generating beta components:  6 13\n",
      "\n",
      " model   best ELL - ELL\n",
      " 6  13   0\n",
      " 7  13   0.04532\n",
      " 5  13   0.04566\n",
      " 4  13   0.105\n",
      " 8  13   0.114\n",
      " 9  13   0.1659\n",
      " 6  12   0.1682\n",
      " 6  14   0.1685\n",
      " 3  13   0.1701\n",
      " 6  11   0.1742\n",
      " 6  15   0.1778\n",
      " 5  11   0.1813\n",
      "13  14   0.1844\n",
      " 7  14   0.1859\n",
      " 7  15   0.1901\n",
      " 5  12   0.1912\n",
      " 2  13   0.2015\n",
      " 7  12   0.2072\n",
      "12  13   0.2073\n",
      " 5  14   0.2092\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top = 20\n",
    "ks = [1, 2, 2]\n",
    "Ds = [10, 20, 20]\n",
    "psis = [8, 8, 4]\n",
    "\n",
    "for (k, D, psi) in zip(ks, Ds, psis):\n",
    "    K = construct_kernel(D, psi=psi)\n",
    "    cov = construct_covariance(K)\n",
    "    Sigma_ff = construct_Sigma_ff(K)\n",
    "    Sigma_Zf = construct_Sigma_Zf(K)\n",
    "    beta, comps = get_beta('{}sparse'.format(k), D)\n",
    "    bSffbp1 = 1 + beta.dot(Sigma_ff).dot(beta)\n",
    "    ELL = lambda s: -.5* (np.log(bSffbp1 - s) + 1 + np.log(2*np.pi))\n",
    "    \n",
    "    scores = {}\n",
    "    for i in range(D):\n",
    "        for j in range(i+1, D):\n",
    "            SZfb = Sigma_Zf[(i,j),:].dot(beta)\n",
    "#             if i == 4:\n",
    "#                 print(cov[:,(i,j)])\n",
    "#                 print(betaTcov)\n",
    "#                 print(cov[np.ix_((i,j),(i,j))])\n",
    "            scores[(i,j)] = np.linalg.solve(cov[np.ix_((i,j),(i,j))], SZfb).dot(SZfb)\n",
    "\n",
    "    scores_arr = np.array([(i1+1,i2+1,s) for (i1,i2), s in scores.items()], \n",
    "                          dtype=[('ind1', int), ('ind2', int), ('score', float)])\n",
    "    scores_arr.sort(order='score')\n",
    "    scores_arr = scores_arr[::-1]\n",
    "    best_ELL = ELL(scores_arr[0][-1])\n",
    "    print('{}-sparse-nonlinear with D = {}, k_max = 2, psi = {}'.format(k, D, psi))\n",
    "    print('best expected log-likelihood:', best_ELL)\n",
    "    print('data-generating beta components: ', *comps)\n",
    "    print()\n",
    "    print(' model   best ELL - ELL')\n",
    "    for ind, (i, j, s) in enumerate(scores_arr):\n",
    "        print('{:2d}{:4d}   {:.4g}'.format(i, j,  best_ELL - ELL(s))) #, bSffbp1, s)\n",
    "        if ind + 1 == top:\n",
    "            break\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
